operation_mode:
  "model_hyperparameter_selection"
app:
  model_config_name:
    "lang_ident_classifier_optim_llama1b_instruct"
  data_dir:
    "data"
  logs_dir:
    "logs"
  pretrained_embeddings_dir:
    "pretrained_embeddings"
  checkpoints_dir:
    "checkpoints"
  random_seed:
    20
  strategy:
    "fsdp"
  num_trials:
    100
run_config:
  accumulate_grad_batches: 8
  training_precision_type: bf16-mixed
  data_sample_share: [1.0]
  pretrained_embedding:
    - meta-llama/Llama-3.2-1B-Instruct
  pretrained_embedding_overwrite_old: False
  max_seq_len: [200, 350, 500]
  words_overlap_percentage: [0.1, 0.25, 0.5]
  batch_size: [4, 8, 16, 32]
  optimizer: ['adamw']
  percentage_of_backbone_model_units_unfrozen: [0, 0.25, 0.5, 0.75, 1.0]
  lora_rank: [4, 8, 12, 16]
  lora_scaling_factor: [4, 8, 12, 16]
  lora_dropout: [0.1, 0.2, 0.3]
  alpha: [0.25, 0.5, 0.75, 0.8, 0.9]
  gamma: [0.0, 0.5, 1.0, 2.0, 5.0]
  loss_type:
    - cross_entropy_loss  
    - class_weighted_cross_entropy_loss
    - focal_loss
    - class_weighted_focal_loss_with_adaptive_focus_type1
    - class_weighted_focal_loss_with_adaptive_focus_type2
    - class_weighted_focal_loss_with_adaptive_focus_type3
  learning_rate: [2e-5, 5e-5, 1e-4]
  num_fc_layers_in_classifier_head: [2]
  activation_function_for_layer: ['relu']
  add_dropout_after_embedding: [True, False]
logs:
  prefix:
    '%Y%m%d'
  suffix:
    'lang_ident_classifier_app.log'
  message_format:
    '[%(asctime)s GMT] %(levelname)s::%(funcName)s() %(message)s'
dataset:
  data_source_dir: "bhasha"
  train_dataset_sample_share:
    0.01
  val_dataset_sample_share:
    0.01
  files_have_header:
    True
  train:
    data_dir:
      "train"
  val:
    data_dir:
      "val"
  test:
    data_dir:
      "test"
model:
  finetune_status:
    False
  max_epochs:
    5 
