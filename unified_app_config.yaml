operation_mode:
  "model_hyperparameter_selection"
app:
  model_config_name:
    "lang_ident_classifier_optim_test"
  data_dir:
    "data"
  logs_dir:
    "logs"
  pretrained_embeddings_dir:
    "pretrained_embeddings"
  checkpoints_dir:
    "checkpoints"
  random_seed:
    20 
  num_trials:
    500
run_config:
  accumulate_grad_batches: 2
  training_precision_type: 16-mixed
  data_sample_share: [1.0]
  pretrained_embedding:
    - Qwen/Qwen2.5-1.5B-Instruct
    #- google/gemma-3-1b-it
    #- meta-llama/Llama-3.2-1B-Instruct
    #- ai4bharat/IndicBERTv2-MLM-only
  pretrained_embedding_overwrite_old: False
  max_seq_len: [500]
  batch_size: [4]
  optimizer: ['adamw']
  num_backbone_model_units_unfrozen: [2]
  loss_type:
    - class_weighted_focal_loss_with_adaptive_focus_type1
  learning_rate: [1e-3]
  num_fc_layers_in_classifier_head: [2]
  activation_function_for_layer: ['parametric_relu']
  add_dropout_after_embedding: [True]
logs:
  prefix:
    '%Y%m%d'
  suffix:
    'lang_ident_classifier_app.log'
  message_format:
    '[%(asctime)s GMT] %(levelname)s::%(funcName)s() %(message)s'
dataset:
  data_source_dir: "bhasha"
  train_dataset_sample_share:
    0.01
  val_dataset_sample_share:
    0.01
  files_have_header:
    True
  train:
    data_dir:
      "train"
  val:
    data_dir:
      "val"
  test:
    data_dir:
      "test"
model:
  finetune_status:
    False
  max_epochs:
    3 
